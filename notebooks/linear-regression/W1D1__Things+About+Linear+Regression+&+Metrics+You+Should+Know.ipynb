{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO LIST\n",
    "\n",
    "---\n",
    "\n",
    "> * Understand the regression algorithms\n",
    "\n",
    "> * Understand the regression evaluation metrics\n",
    "\n",
    "> * Prepare/transform the dataset to try to improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "---\n",
    "\n",
    "The goal of linear regression is to model the relationship between one or multiple features and a continuous target variable. Regression analysis is a subcategory of supervised machine learning. In contrast to classification— another subcategory of supervised learning—regression analysis aims to predict outputs on a continuous scale rather than categorical class labels.\n",
    "\n",
    "<center><img src=\"15.jpg\"></center>\n",
    "\n",
    "\n",
    "## Simple linear regression\n",
    "\n",
    "---\n",
    "\n",
    "The goal of simple (univariate) linear regression is to model the relationship between a single feature (explanatory variable x) and a continuous valued response (target variable y). The equation of a linear model with one explanatory variable is defined as follows:\n",
    "<center><img src=\"lr.jpg\">\n",
    "\n",
    "Here, the weight $$ {W}_{0} $$ represents the y-axis intercept and $$ {W}_{1} $$ is the weight coefficient of the explanatory variable. Our goal is to learn the weights of the linear equation to describe the relationship between the explanatory (independent) variable and the target variable, which can then be used to predict the responses of new explanatory variables that\n",
    "were not part of the training dataset. Based on the linear equation that we defined previously, linear regression can be\n",
    "understood as finding the best-fitting straight line through the sample points, as shown in the following figure:\n",
    "<center><img src=\"scat.jpg\">\n",
    "\n",
    "This best-fitting line is also called the regression line, and the vertical lines from the regression line to the sample points are the so-called offsets or residuals - the errors of our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "---\n",
    "\n",
    "The special case of linear regression with one explanatory variable that we introduced in the previous subsection is called simple linear regression. Of course, we can also generalize the linear regression model to multiple explanatory\n",
    "variables; this process is called multiple linear regression.\n",
    "<center><img src=\"mlr.jpg\">\n",
    "\n",
    "Here, $$ {W}_{0} $$ is the y-axis intercept with $$ {X}_{0} = 1 $$\n",
    "\n",
    "The following figure shows how the two-dimensional, fitted hyperplane of a multiple linear regression model with two features could look like\n",
    "<center><img src=\"mm.jpg\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we find the best fit line?\n",
    "\n",
    "---\n",
    "\n",
    "We define a **cost function** as:\n",
    "\n",
    "<center><img src=\"cf.jpg\">\n",
    "\n",
    "This is also known as **Sum of Squared Errors(SSE)**. Let's try to understand the intuition behind cost function\n",
    "\n",
    "<center><img src=\"1.jpg\">\n",
    "\n",
    "Now, no brainer the best fit line would be $$ Y = X $$ which would mean $$ {W}_{0} = 0 $$ and $$ {W}_{1} = 1 $$\n",
    "\n",
    "<center><img src=\"2.jpg\">\n",
    "\n",
    "Now if we calculate the cost function for $$ {W}_{0} = 0 $$ and $$ {W}_{1} = 1 $$ the value would be $$ J(w) = ((1 - 1) ^ 2 + (2 - 2) ^ 2 + (3 - 3) ^ 2)/2 = 0 $$\n",
    "\n",
    "But if we calculate the cost function for $$ {W}_{0} = 0 $$ and $$ {W}_{1} = 0.5 $$ the line fitted would be like \n",
    "\n",
    "<center><img src=\"3.JPG\">\n",
    "\n",
    "And the cost function value would be $$ J(w) = ((1 - 0.5) ^ 2 + (2 - 1) ^ 2 + (3 - 1.5) ^ 2)/2 = 1.75 $$\n",
    "\n",
    "So overall, the relationship between the weights (& intercept) and the cost function looks like\n",
    "\n",
    "<center><img src=\"cp.jpg\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, how do we find the minimum point?\n",
    "\n",
    "---\n",
    "\n",
    "We can use a technique called **Gradient Descent (GD)**\n",
    "\n",
    "<center><font color=\"red\">Repeat until convergence</font></center>\n",
    "\n",
    "<center><img src=\"4.jpg\"></center>\n",
    "\n",
    "<center><img src=\"5.jpg\"></center>\n",
    "\n",
    "---\n",
    "\n",
    "## Learning rate\n",
    "\n",
    "---\n",
    "\n",
    "Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function\n",
    "\n",
    "## How do we decide the learning rate?\n",
    "\n",
    "<center><img src=\"6.jpg\">\n",
    "\n",
    "<center><img src=\"7.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Other ways of finding the best fit line: LIBLINEAR (SCIKIT-LEARN), RANSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "---\n",
    "\n",
    "> **Mean Absolute Error(MAE)**\n",
    "\n",
    "MAE is a very simple metric which calculates the absolute difference between actual and predicted values.\n",
    "\n",
    "To better understand, let’s take an example you have input data and output data and use Linear Regression, which draws a best-fit line.\n",
    "\n",
    "Now you have to find the MAE of your model which is basically a mistake made by the model known as an error. Now find the difference between the actual value and predicted value that is an absolute error but we have to find the mean absolute of the complete dataset.\n",
    "\n",
    "so, sum all the errors and divide them by a total number of observations And this is MAE. And we aim to get a minimum MAE because this is a loss.\n",
    "\n",
    "\n",
    "<center><img src=\"8.jpg\"></center>\n",
    "\n",
    "> **Mean Squared Error(MSE)**\n",
    "\n",
    "MSE is a most used and very simple metric with a little bit of change in mean absolute error. Mean squared error states that finding the squared difference between actual and predicted value.\n",
    "\n",
    "So, above we are finding the absolute difference and here we are finding the squared difference.\n",
    "\n",
    "What actually the MSE represents? It represents the squared distance between actual and predicted values. we perform squared to avoid the cancellation of negative terms and it is the benefit of MSE.\n",
    "\n",
    "<center><img src=\"9.jpg\"></center>\n",
    "\n",
    "> **Root Mean Squared Error(RMSE)**\n",
    "\n",
    "As RMSE is clear by the name itself, that it is a simple square root of mean squared error.\n",
    "\n",
    "<center><img src=\"10.jpg\"></center>\n",
    "\n",
    "\n",
    "> **Coefficient of Determination (R-squared)**\n",
    "\n",
    "R2 score is a metric that tells the performance of your model, not the loss in an absolute sense that how many wells did your model perform.\n",
    "\n",
    "In contrast, MAE and MSE depend on the context as we have seen whereas the R2 score is independent of context.\n",
    "\n",
    "So, with help of R squared we have a baseline model to compare a model which none of the other metrics provides. The same we have in classification problems which we call a threshold which is fixed at 0.5. So basically R2 squared calculates how must regression line is better than a mean line.\n",
    "\n",
    "Hence, R2 squared is also known as Coefficient of Determination or sometimes also known as Goodness of fit.\n",
    "\n",
    "<center><img src=\"11.jpg\"></center>\n",
    "\n",
    "where\n",
    "<center><img src=\"12.jpg\"></center>\n",
    "and\n",
    "<center><img src=\"13.jpg\"></center>\n",
    "\n",
    "> **Adjusted Coefficient of Determination (Adjusted R-squared)**\n",
    "\n",
    "The disadvantage of the R2 score is while adding new features in data the R2 score starts increasing or remains constant but it never decreases because It assumes that while adding more data variance of data increases.\n",
    "\n",
    "But the problem is when we add an irrelevant feature in the dataset then at that time R2 sometimes starts increasing which is incorrect.\n",
    "\n",
    "Hence, To control this situation Adjusted R Squared came into existence.\n",
    "\n",
    "<center><img src=\"14.jpg\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# mean_absolute_error(<actual target>, <predictions>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# mean_squared_error(<actual target>, <predictions>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# np.sqrt(mean_squared_error(<actual target>, <predictions>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "# r2_score(<actual target>, <predictions>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n=<number_of_records>\n",
    "# k=<number_of_indep_variables>\n",
    "# r2 = r2_score(<actual target>, <predictions>)\n",
    "# adj_r2_score = 1 - ((1-r2)*(n-1)/(n-k-1))\n",
    "# print(adj_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming up next\n",
    "\n",
    "---\n",
    "\n",
    "> * Key assumptions to test before fitting linear regression\n",
    "\n",
    "> * Transformation requirements for linear regression\n",
    "\n",
    "> * Application & evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
