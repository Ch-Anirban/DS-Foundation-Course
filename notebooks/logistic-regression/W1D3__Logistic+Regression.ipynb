{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "> Logistic regression is a classification model that is very easy to implement but performs very well on linearly separable classes. It is one of the most widely used algorithms for classification in industry.\n",
    "\n",
    "> To explain the idea behind logistic regression as a probabilistic model, let's first introduce the odds ratio: the odds in favor of a particular event. The odds ratio can be written as $$ \\frac{p}{1-p} $$ where $$ p $$ stands for the probability of the positive event. \n",
    "\n",
    "> **The term positive event does not necessarily mean good, but refers to the event that we want to predict, for example, the probability that a patient has a certain disease**; we can think of the positive event as class label . We can then further define the logit function, which is simply the logarithm of the odds ratio (log-odds):\n",
    "\n",
    "$$ logit(p) = \\log\\frac{p}{1-p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The logit function takes as input values in the **range 0 to 1** and transforms them to values over the **entire real-number range**, which we can use to express a linear relationship between feature values and the log-odds:\n",
    "\n",
    "<center><img src=\"1.jpg\">\n",
    "\n",
    "> Here, is the **conditional probability** that a particular sample belongs to class 1 given its features x.\n",
    "\n",
    "> Now, we are actually interested in predicting the probability that a certain sample belongs to a particular class, which is the inverse form of the logit function. It is also called logistic sigmoid function, sometimes simply abbreviated to sigmoid function due to its characteristic S-shape\n",
    "\n",
    "<center><img src=\"2.jpg\">\n",
    "\n",
    "> Here z is the net input, the linear combination of weights and sample features, $$ z = W^TX $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To explain how we can derive the cost function for logistic regression, let's first define the likelihood L that we want to maximize when we build a logistic regression model, assuming that the individual samples in our dataset are independent of one another. The formula is as follows:\n",
    "\n",
    "<center><img src=\"4.jpg\"></center>\n",
    "\n",
    "> In practice, it is easier to maximize the (natural) log of this equation, which is called\n",
    "the log-likelihood function:\n",
    "\n",
    "<center><img src=\"5.jpg\"></center>\n",
    "\n",
    "> Now we could use an optimization algorithm such as gradient ascent to maximize this log-likelihood function. Alternatively, let's rewrite the log-likelihood as a cost function J that can be minimized using gradient descent\n",
    "\n",
    "<center><img src=\"6.jpg\"></center>\n",
    "\n",
    "> A simplified version to remember\n",
    "\n",
    "<center><img src=\"7.jpg\"></center>\n",
    "\n",
    ">  plotting that illustrates the cost of classifyinga single-sample instance for different values of $$ \\phi(z) $$\n",
    "\n",
    "<center><img src=\"8.jpg\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Classification\n",
    "\n",
    "---\n",
    "\n",
    "### Problem statement for illustration\n",
    "\n",
    "<center><img src=\"9.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = [0,0,1,0,1,1,1,0,1,0]\n",
    "y_pred = [0,1,1,0,0,1,1,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2],\n",
       "       [1, 4]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model says 0</th>\n",
       "      <th>model says 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>we know 0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we know 1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model says 0  model says 1\n",
       "we know 0             3             2\n",
       "we know 1             1             4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "cm = DataFrame(confusion_matrix(y_true, y_pred), columns=['model says 0', 'model says 1'], index=['we know 0', 'we know 1'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "we know 0    5\n",
       "we know 1    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model says 0    4\n",
       "model says 1    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"10.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **true positive (TP)**\n",
    "\n",
    "> A test result that correctly indicates the presence of a condition or characteristic\n",
    "\n",
    "> **true negative (TN)**\n",
    "\n",
    "> A test result that correctly indicates the absence of a condition or characteristic\n",
    "\n",
    "> **false positive (FP)**\n",
    "\n",
    "> A test result which wrongly indicates that a particular condition or attribute is present\n",
    "\n",
    "> **false negative (FN)**\n",
    "\n",
    "> A test result which wrongly indicates that a particular condition or attribute is absent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"12.jpg\">\n",
    "<center><img src=\"11.jpg\">\n",
    "<center><img src=\"13.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "precision_score(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727272"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve & AUC\n",
    "\n",
    "Receiver Operating Characteristic (ROC) graphs are useful tools to select models for classification based on their performance with respect to the FPR and TPR, which are computed by shifting the decision threshold of the classifier. The diagonal of an ROC graph can be interpreted as random guessing, and classification models that fall below the diagonal are considered as worse than random guessing. A perfect classifier would fall into the top left corner of the graph with a TPR of 1 and an FPR of 0. Based on the ROC curve, we can then compute the so-called ROC Area Under the Curve (ROC AUC) to characterize the performance of a classification model.\n",
    "\n",
    "<center><img src=\"14.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
